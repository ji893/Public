import os
import tempfile
import hashlib
import streamlit as st
from dotenv import load_dotenv

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

#  OpenAI API Key ì„¤ì •
load_dotenv()

# Streamlit UI êµ¬ì„±
st.set_page_config(page_title="íŒŒì¼ ì—…ë¡œë“œ + í—Œë²• Q&A ì±—ë´‡", layout="centered")
                                                                            
st.header(" ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ Q&A ì±—ë´‡ ")

# GPT ëª¨ë¸ ì„ íƒ
selected_model = st.selectbox("ì‚¬ìš©í•  GPT ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:", ("gpt-4o", "gpt-3.5-turbo-0125"))

# PDF ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ğŸ“ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

# ğŸ”‘ PDF í•´ì‹œ ìƒì„± í•¨ìˆ˜
def get_file_hash(file) -> str:  
    content = file.read()
    file.seek(0) 
    return hashlib.md5(content).hexdigest()

# PDF ë¡œë“œ ë° ë¶„í• 

@st.cache_resource 
def load_and_split_pdf(file) -> list:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file.read())
        tmp_file_path = tmp_file.name
    loader = PyPDFLoader(tmp_file_path)
    return loader.load_and_split()

# FAISS ì €ì¥/ë¡œë“œ í†µí•© í•¨ìˆ˜
@st.cache_resource  
def load_or_create_vectorstore(_docs, file_hash):  
    index_path = os.path.join("faiss_index", file_hash) 
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

    if os.path.exists(index_path):  
        return FAISS.load_local(index_path, embedding_model) 
    # ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0) 
    split_docs = text_splitter.split_documents(_docs)
    for doc in split_docs:
        doc.metadata["source"] = f"{doc.metadata.get('source', 'ì—…ë¡œë“œ íŒŒì¼')} (p.{doc.metadata.get('page', 'n/a')})" 
    vectorstore = FAISS.from_documents(split_docs, embedding_model)

    os.makedirs("faiss_index", exist_ok=True) 
    vectorstore.save_local(index_path) 
    return vectorstore

# RAG ì²´ì¸ êµ¬ì„±
def initialize_rag_chain(docs, file_hash, selected_model):
    vectorstore = load_or_create_vectorstore(docs, file_hash)
    retriever = vectorstore.as_retriever()

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and a new question, return a standalone version of the question."),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say you don't know. 
Use polite Korean and include emoji.\n\n{context}"""),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])

    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# 
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš” ğŸ˜Š"}]

# 
if uploaded_file:
    file_hash = get_file_hash(uploaded_file) 
    
    with st.spinner("PDF ë¶„ì„ ì¤‘..."): 
        pages = load_and_split_pdf(uploaded_file) 
        
        rag_chain = initialize_rag_chain(pages, file_hash, selected_model)
        
        chat_history = StreamlitChatMessageHistory(key="chat_messages")
        conversational_chain = RunnableWithMessageHistory(
            rag_chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history",
            output_messages_key="answer",
        )


    for msg in chat_history.messages:
        st.chat_message(msg.type).write(msg.content)

    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.chat_message("human").write(prompt)
        with st.chat_message("ai"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                config = {"configurable": {"session_id": "upload_session"}}
                response = conversational_chain.invoke({"input": prompt}, config)
                answer = response["answer"]
                st.write(answer)

                with st.expander(" ì°¸ê³ í•œ ë¬¸ì„œ ë³´ê¸°"):
                    for doc in response.get("context", []):
                        st.markdown(f"{doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}", help=doc.page_content)


